# commonly used algorithms



<!--
are:
 1. Linear Regression - A simple and widely-used algorithm for regression problems, which finds the  
best fit line through a set of data points by minimizing the sum squared errors between predicted values and
actual ones.
2. Logistic Regression (Binary Classification) - This is another popular machine learning technique that can
be 
applied to binary classification tasks such as predicting whether an email is spam or not based on its text
content. It works by fitting a logistic function to the input features using maximum likelihood estimation techniques like
content. It works by fitting a logistic function onto the input features using maximum likelihood estimation methodology
content. It works by fitting a logistic function onto the input features using maximum likelihood estimation methodology
content. It works by fitting a logistic function onto the input features using maximum likelihood estimation methodology
content. It works by fitting a logistic function onto the input features using maximum likelihood estimation techniques like
content. It works by fitting a logistic function to model the probability of each class label given input
features.
The output from this function represents probabilities in [0,1], with higher numbers indicating greater confidence levels
The output value from this function represents the likelihood of belonging to either class.
If it's greater than
some threshold, we classify it into one category; otherwise, we move onto other categories.
3. Decision Trees / Random Forests - These are both ensemble methods that combine multiple decision
trees in order
to improve their accuracy over individual trees. The main difference lies in how they handle
feature
selection during training time. In random forests, features are selected randomly at every split point instead of
selection during training time. In random forests, instead of selecting only the most important feature at every
selection during training time. In random forests, instead of selecting features randomly at every node split,
selection during training time. In random forests, features are selected randomly at every split node instead of
selection during training time. In random forests, features are selected randomly at every split point instead of
selection during training time. In random forests, different subsets of features are selected at -->